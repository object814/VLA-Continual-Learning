{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of OpenVLA Model on LIBERO Dataset\n",
    "\n",
    "This notebook evaluates the process of using the OpenVLA model on one task from the LIBERO dataset. The following steps are performed:\n",
    "\n",
    "1. **Setup and Imports**: Import necessary libraries and set up the environment.\n",
    "2. **Load Processor and Model**: Load the OpenVLA processor and model from HuggingFace.\n",
    "3. **Load LIBERO Dataset Configuration**: Load the configuration for the LIBERO dataset.\n",
    "4. **Prepare Datasets**: Prepare datasets from the LIBERO benchmark.\n",
    "5. **Extract Sample Data and Process Inputs**: Extract a sample image and instruction from the LIBERO dataset, process the inputs using the OpenVLA processor, visualize the raw RGB image, print the raw instruction and formatted prompt, print the size of the processed input tensors, and print the OpenVLA model outputs for each step.\n",
    "\n",
    "By running these sections sequentially, we can evaluate the whole process for one task from the LIBERO dataset, visualize the raw RGB image, print the instructions and prompts, show the input tensor sizes, and print the OpenVLA model outputs for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from easydict import EasyDict\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from libero.lifelong.datasets import get_dataset, SequenceVLDataset\n",
    "from libero.lifelong.utils import get_task_embs\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Load Processor and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Load LIBERO Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "from hydra.experimental import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "from easydict import EasyDict\n",
    "import yaml\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "\n",
    "# Check if Hydra is already initialized\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "# Load LIBERO dataset configuration\n",
    "initialize(config_path=\"../external/LIBERO/libero/configs\")\n",
    "hydra_cfg = compose(config_name=\"config\")\n",
    "yaml_config = OmegaConf.to_yaml(hydra_cfg)\n",
    "cfg = EasyDict(yaml.safe_load(yaml_config))\n",
    "\n",
    "# Prepare lifelong learning configuration\n",
    "cfg.folder = get_libero_path(\"datasets\")\n",
    "cfg.benchmark_name = \"libero_object\"\n",
    "task_order = cfg.data.task_order_index\n",
    "benchmark = get_benchmark(cfg.benchmark_name)(task_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: rgb with keys: ['eye_in_hand_rgb', 'agentview_rgb']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: low_dim with keys: ['joint_states', 'gripper_states']\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 634.87it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 673.51it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 666.07it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 701.91it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 689.14it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 693.39it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 713.40it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 696.57it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 668.78it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 699.46it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhaoyu/anaconda3/envs/VLA_CL/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets from the benchmark\n",
    "datasets = []\n",
    "descriptions = []\n",
    "shape_meta = None\n",
    "n_tasks = benchmark.n_tasks\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    task_i_dataset, shape_meta = get_dataset(\n",
    "        dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)),\n",
    "        obs_modality=cfg.data.obs.modality,\n",
    "        initialize_obs_utils=(i==0),\n",
    "        seq_len=cfg.data.seq_len,\n",
    "    )\n",
    "    descriptions.append(benchmark.get_task(i).language)\n",
    "    datasets.append(task_i_dataset)\n",
    "\n",
    "task_embs = get_task_embs(cfg, descriptions)\n",
    "benchmark.set_task_embs(task_embs)\n",
    "datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<libero.lifelong.datasets.SequenceVLDataset object at 0x7f922c713460>\n",
      "['__add__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', 'n_demos', 'sequence_dataset', 'task_emb', 'total_num_sequences']\n"
     ]
    }
   ],
   "source": [
    "print(datasets[0])\n",
    "print(dir(datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Extract Sample Data and Process Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'observations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      7\u001b[0m     sample_data \u001b[38;5;241m=\u001b[39m sample_dataset[step]\n\u001b[0;32m----> 8\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43msample_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobservations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Example of extracting the first RGB frame\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m descriptions[sample_task_idx]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Convert the image to a PIL Image\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'observations'"
     ]
    }
   ],
   "source": [
    "# Extract a sample image and instruction from the LIBERO dataset\n",
    "sample_task_idx = 0\n",
    "sample_dataset = datasets[sample_task_idx]\n",
    "\n",
    "# Extract the first demonstration and process the first 10 steps\n",
    "for step in range(10):\n",
    "    sample_data = sample_dataset[step]\n",
    "    image = sample_data['observations']['rgb'][0]  # Example of extracting the first RGB frame\n",
    "    instruction = descriptions[sample_task_idx]\n",
    "\n",
    "    # Convert the image to a PIL Image\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # Format the prompt with the instruction\n",
    "    prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n",
    "    # Process the inputs\n",
    "    inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "    \n",
    "    # Visualize the raw RGB image\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Step {step+1} - RGB Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the raw instruction and formatted prompt\n",
    "    print(f\"Step {step+1} - Raw Instruction: {instruction}\")\n",
    "    print(f\"Step {step+1} - Formatted Prompt: {prompt}\")\n",
    "\n",
    "    # Print the size of the processed input tensors\n",
    "    print(f\"Step {step+1} - Input Tensor Shapes:\")\n",
    "    for key, value in inputs.items():\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "    # Predict action using OpenVLA model\n",
    "    action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "    # Print the predicted action\n",
    "    print(f\"Step {step+1} - Predicted Action: {action}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla_cl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

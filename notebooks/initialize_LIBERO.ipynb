{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of OpenVLA Model on LIBERO Dataset\n",
    "\n",
    "This notebook evaluates the process of using the OpenVLA model on one task from the LIBERO dataset. The following steps are performed:\n",
    "\n",
    "1. **Setup and Imports**: Import necessary libraries and set up the environment.\n",
    "2. **Load Processor and Model**: Load the OpenVLA processor and model from HuggingFace.\n",
    "3. **Load LIBERO Dataset Configuration**: Load the configuration for the LIBERO dataset.\n",
    "4. **Prepare Datasets**: Prepare datasets from the LIBERO benchmark.\n",
    "5. **Extract Sample Data and Process Inputs**: Extract a sample image and instruction from the LIBERO dataset, process the inputs using the OpenVLA processor, visualize the raw RGB image, print the raw instruction and formatted prompt, print the size of the processed input tensors, and print the OpenVLA model outputs for each step.\n",
    "\n",
    "By running these sections sequentially, we can evaluate the whole process for one task from the LIBERO dataset, visualize the raw RGB image, print the instructions and prompts, show the input tensor sizes, and print the OpenVLA model outputs for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Load VLA huggingface Processor and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data2/zhaoyu/huggingface_cache'\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "\n",
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:2\")\n",
    "print(vla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Load LIBERO Demonstration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset structure:\n",
    "language_instruction: a string of language instruction for the task\n",
    "actions_batch: numpy array with size: (50, N, 8)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of actions in each demonstration\n",
    "    - 8: action dimension\n",
    "images_batch: numpy array with size: (50, N, 128, 128, 3)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of images in each demonstration\n",
    "    - 128x128: image size\n",
    "    - 3: RGB\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_10\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "# currently no need to change FILTER_KEY and VERBOSE\n",
    "FILTER_KEY = None  # Set filter key if needed, e.g., \"valid\" for validation\n",
    "VERBOSE = True\n",
    "\n",
    "## Check libero dataset path\n",
    "BENCHMARK_PATH = get_libero_path(\"benchmark_root\")\n",
    "DATASET_BASE_PATH = get_libero_path(\"datasets\")\n",
    "DATASET_PATH_DEMO = os.path.join(DATASET_BASE_PATH, DATASET_NAME)\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO benchmark root path: \", BENCHMARK_PATH)\n",
    "print(\"LIBERO dataset root path: \", DATASET_BASE_PATH)\n",
    "print(f\"LIBERO demonstration dataset for {DATASET_NAME} path: {DATASET_PATH_DEMO}\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "## Load demonstration dataset\n",
    "# get all task names in the dataset\n",
    "task_names_demo = get_task_names(DATASET_PATH_DEMO)\n",
    "# print(f\"Tasks in the demonstration dataset: {task_names_demo}\")\n",
    "# load demonstration data for each task\n",
    "dataset_demo = {}\n",
    "print(\"Start loading demonstration data for each task...\")\n",
    "print(\"-------------------------------------\")\n",
    "for task_name_demo in task_names_demo:\n",
    "    print(f\"Loading demonstration data for task:\\n {task_name_demo}\")\n",
    "    [language_instruction, actions_batch, images_batch] = extract_task_info(DATASET_PATH_DEMO, task_name_demo, filter_key=FILTER_KEY, verbose=VERBOSE)\n",
    "    dataset_demo[task_name_demo] = [language_instruction, actions_batch, images_batch]\n",
    "    # check if actions_batch and images_batch have the same length\n",
    "    assert actions_batch.shape[0] == images_batch.shape[0], \"Dataset problem: the number of actions and images should be the same!\"\n",
    "    # print dataset information\n",
    "    print(\"Loaded successfully!\")\n",
    "    print(f\"Total demonstrations: {actions_batch.shape[0]}\")\n",
    "    ave_len = np.mean([len(x) for x in actions_batch]) # average length of demonstrations\n",
    "    print(f\"Average demonstration length: {ave_len}\")\n",
    "    action_shape = actions_batch[0][0].shape # action shape\n",
    "    print(f\"Action shape: {action_shape}\")\n",
    "    img_shape = images_batch[0][0].shape # image shape\n",
    "    print(f\"Image shape: {img_shape}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Train OpenVLA on LIBERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Evaluate OpenVLA on Trained LIBERO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUJOCO_GL=osmesa\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATASET_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Load evaluation dataset\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# task_names_demo = list(demonstration_data.keys())\u001b[39;00m\n\u001b[1;32m     44\u001b[0m benchmark_dict \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mget_benchmark_dict()\n\u001b[0;32m---> 45\u001b[0m benchmark_instance \u001b[38;5;241m=\u001b[39m benchmark_dict[\u001b[43mDATASET_NAME\u001b[49m]()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# num_tasks_eval = benchmark_instance.get_num_tasks()\u001b[39;00m\n\u001b[1;32m     47\u001b[0m task_names_eval \u001b[38;5;241m=\u001b[39m benchmark_instance\u001b[38;5;241m.\u001b[39mget_task_names()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATASET_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "%env MUJOCO_GL=osmesa\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "# from libero.lifelong.metric import (\n",
    "#     evaluate_loss,\n",
    "#     evaluate_success,\n",
    "#     raw_obs_to_tensor_obs,\n",
    "# )\n",
    "# from libero.lifelong.utils import (\n",
    "#     control_seed,\n",
    "#     safe_device,\n",
    "#     torch_load_model,\n",
    "#     NpEncoder,\n",
    "#     compute_flops,\n",
    "# )\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from libero.libero.envs import OffScreenRenderEnv, SubprocVectorEnv\n",
    "\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info, extract_env_obs\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_10\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "SAVE_VIDEO = False # save video of the evaluation process\n",
    "EVAL_MAX_STEP = 500 # maximum number of steps for evaluation\n",
    "DEVICE_ID = 2 # GPU device id for rendering\n",
    "\n",
    "## Check evaluation configureations path\n",
    "BDDL_FILES_BASE_PATH = get_libero_path(\"bddl_files\")\n",
    "INIT_STATES_BASE_PATH = get_libero_path(\"init_states\")\n",
    "VIDEO_FOLDER = \"../videos\"\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO evaluation BDDL files path: \", BDDL_FILES_BASE_PATH)\n",
    "print(\"LIBERO evaluation initial states path: \", INIT_STATES_BASE_PATH)\n",
    "if SAVE_VIDEO:\n",
    "    print(\"Video saving to: \", VIDEO_FOLDER)\n",
    "print(\"=====================================\")\n",
    "\n",
    "\n",
    "## Load evaluation environment\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "benchmark_instance = benchmark_dict[DATASET_NAME]()\n",
    "# num_tasks_eval = benchmark_instance.get_num_tasks()\n",
    "task_names_eval = benchmark_instance.get_task_names()\n",
    "print(f\"Task names for evaluation: {task_names_eval}\")\n",
    "# print(f\"Task name for training: {task_names_demo}\")\n",
    "\n",
    "# Evaluate the model\n",
    "task_id = 2\n",
    "task = benchmark_instance.get_task(task_id)\n",
    "\n",
    "with Timer() as t, VideoWriter(VIDEO_FOLDER, SAVE_VIDEO) as video_writer:\n",
    "    env_args = {\n",
    "        \"bddl_file_name\": os.path.join(\n",
    "            BDDL_FILES_BASE_PATH, task.problem_folder, task.bddl_file\n",
    "        ),\n",
    "        \"render_gpu_device_id\": DEVICE_ID\n",
    "    }\n",
    "\n",
    "    env_num = 3\n",
    "    env = SubprocVectorEnv(\n",
    "        [lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)]\n",
    "    )\n",
    "        \n",
    "    env.reset()\n",
    "    env.seed(random.randint(0, 1000))\n",
    "\n",
    "    init_states_path = os.path.join(\n",
    "        INIT_STATES_BASE_PATH, task.problem_folder, task.init_states_file\n",
    "    )\n",
    "    init_states = torch.load(init_states_path)\n",
    "    indices = np.arange(env_num) % init_states.shape[0]\n",
    "    init_states_ = init_states[indices]\n",
    "\n",
    "    dones = [False] * env_num\n",
    "    steps = 0\n",
    "    obs = env.set_init_state(init_states_)\n",
    "    print(\"Initial RGB observations:\")\n",
    "    stacked_image = np.hstack([obs[k][\"agentview_image\"][::-1] for k in range(env_num)])\n",
    "    plt.imshow(stacked_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    num_success = 0\n",
    "    for _ in range(5):  # simulate the physics without any actions\n",
    "        obs, reward, done, info = env.step(np.zeros((env_num, 7)))\n",
    "        \n",
    "    print(\"RGB observations after 5 steps:\")\n",
    "    stacked_image = np.hstack([obs[k][\"agentview_image\"][::-1] for k in range(env_num)])\n",
    "    plt.imshow(stacked_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         while steps < EVAL_MAX_STEP:\n",
    "#             steps += 1\n",
    "\n",
    "#             # get current observation\n",
    "            \n",
    "#             data = extract_env_obs(obs, DEVICE_ID)\n",
    "#             print(data)\n",
    "#             input(\"PAUSE\")\n",
    "#             # actions = algo.policy.get_action(data)\n",
    "#             obs, reward, done, info = env.step(actions)\n",
    "#             video_writer.append_vector_obs(\n",
    "#                 obs, dones, camera_name=\"agentview_image\"\n",
    "#             )\n",
    "\n",
    "#             # check whether succeed\n",
    "#             for k in range(env_num):\n",
    "#                 dones[k] = dones[k] or done[k]\n",
    "#             if all(dones):\n",
    "#                 break\n",
    "\n",
    "#         for k in range(env_num):\n",
    "#             num_success += int(dones[k])\n",
    "\n",
    "#     success_rate = num_success / env_num\n",
    "#     env.close()\n",
    "\n",
    "#     eval_stats = {\n",
    "#         \"loss\": test_loss,\n",
    "#         \"success_rate\": success_rate,\n",
    "#     }\n",
    "\n",
    "#     os.system(f\"mkdir -p {args.save_dir}\")\n",
    "#     torch.save(eval_stats, save_folder)\n",
    "# print(\n",
    "#     f\"[info] finish for ckpt at {run_folder} in {t.get_elapsed_time()} sec for rollouts\"\n",
    "# )\n",
    "# print(f\"Results are saved at {save_folder}\")\n",
    "# print(test_loss, success_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla_cl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

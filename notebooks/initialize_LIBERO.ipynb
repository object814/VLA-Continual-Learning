{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of OpenVLA Model on LIBERO Dataset\n",
    "\n",
    "This notebook evaluates the process of using the OpenVLA model on one task from the LIBERO dataset. The following steps are performed:\n",
    "\n",
    "1. **Setup and Imports**: Import necessary libraries and set up the environment.\n",
    "2. **Load Processor and Model**: Load the OpenVLA processor and model from HuggingFace.\n",
    "3. **Load LIBERO Dataset Configuration**: Load the configuration for the LIBERO dataset.\n",
    "4. **Prepare Datasets**: Prepare datasets from the LIBERO benchmark.\n",
    "5. **Extract Sample Data and Process Inputs**: Extract a sample image and instruction from the LIBERO dataset, process the inputs using the OpenVLA processor, visualize the raw RGB image, print the raw instruction and formatted prompt, print the size of the processed input tensors, and print the OpenVLA model outputs for each step.\n",
    "\n",
    "By running these sections sequentially, we can evaluate the whole process for one task from the LIBERO dataset, visualize the raw RGB image, print the instructions and prompts, show the input tensor sizes, and print the OpenVLA model outputs for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from easydict import EasyDict\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from libero.lifelong.datasets import get_dataset, SequenceVLDataset\n",
    "from libero.lifelong.utils import get_task_embs\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Load Processor and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Load LIBERO Demonstration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset path\n",
    "BENCHMARK_PATH = get_libero_path(\"benchmark_root\")\n",
    "DATASET_BASE_PATH = get_libero_path(\"datasets\")\n",
    "print(\"Default benchmark root path: \", BENCHMARK_PATH)\n",
    "print(\"Default dataset root path: \", DATASET_BASE_PATH)\n",
    "\n",
    "# Select a dataset\n",
    "DATASET_NAME = \"libero_object\"\n",
    "FILTER_KEY = None  # Set filter key if needed, e.g., \"valid\" for validation\n",
    "VERBOSE = True\n",
    "dataset_path = os.path.join(DATASET_BASE_PATH, DATASET_NAME)\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Load dataset\n",
    "# get all task names in the dataset\n",
    "task_names = get_task_names(dataset_path)\n",
    "for task_name in task_names:\n",
    "    print(f\"Task: {task_name}\")\n",
    "    [language_instruction, actions_batch, images_batch] = extract_task_info(dataset_path, task_name, filter_key=FILTER_KEY, verbose=VERBOSE)\n",
    "    print(language_instruction)\n",
    "    print(actions_batch[0])\n",
    "    print(images_batch[0])\n",
    "    input(\"continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets from the benchmark\n",
    "datasets = []\n",
    "descriptions = []\n",
    "shape_meta = None\n",
    "n_tasks = benchmark.n_tasks\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    task_i_dataset, shape_meta = get_dataset(\n",
    "        dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)),\n",
    "        obs_modality=cfg.data.obs.modality,\n",
    "        initialize_obs_utils=(i==0),\n",
    "        seq_len=cfg.data.seq_len,\n",
    "    )\n",
    "    descriptions.append(benchmark.get_task(i).language)\n",
    "    datasets.append(task_i_dataset)\n",
    "\n",
    "task_embs = get_task_embs(cfg, descriptions)\n",
    "benchmark.set_task_embs(task_embs)\n",
    "datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[0])\n",
    "print(dir(datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Extract Sample Data and Process Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a sample image and instruction from the LIBERO dataset\n",
    "sample_task_idx = 0\n",
    "sample_dataset = datasets[sample_task_idx]\n",
    "\n",
    "# Extract the first demonstration and process the first 10 steps\n",
    "for step in range(10):\n",
    "    sample_data = sample_dataset[step]\n",
    "    image = sample_data['observations']['rgb'][0]  # Example of extracting the first RGB frame\n",
    "    instruction = descriptions[sample_task_idx]\n",
    "\n",
    "    # Convert the image to a PIL Image\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # Format the prompt with the instruction\n",
    "    prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n",
    "    # Process the inputs\n",
    "    inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "    \n",
    "    # Visualize the raw RGB image\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Step {step+1} - RGB Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the raw instruction and formatted prompt\n",
    "    print(f\"Step {step+1} - Raw Instruction: {instruction}\")\n",
    "    print(f\"Step {step+1} - Formatted Prompt: {prompt}\")\n",
    "\n",
    "    # Print the size of the processed input tensors\n",
    "    print(f\"Step {step+1} - Input Tensor Shapes:\")\n",
    "    for key, value in inputs.items():\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "    # Predict action using OpenVLA model\n",
    "    action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "    # Print the predicted action\n",
    "    print(f\"Step {step+1} - Predicted Action: {action}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla_cl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

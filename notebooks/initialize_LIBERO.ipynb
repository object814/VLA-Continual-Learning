{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of OpenVLA Model on LIBERO Dataset\n",
    "\n",
    "This notebook evaluates the process of using the OpenVLA model on one task from the LIBERO dataset. The following steps are performed:\n",
    "\n",
    "1. **Setup and Imports**: Import necessary libraries and set up the environment.\n",
    "2. **Load Processor and Model**: Load the OpenVLA processor and model from HuggingFace.\n",
    "3. **Load LIBERO Dataset Configuration**: Load the configuration for the LIBERO dataset.\n",
    "4. **Prepare Datasets**: Prepare datasets from the LIBERO benchmark.\n",
    "5. **Extract Sample Data and Process Inputs**: Extract a sample image and instruction from the LIBERO dataset, process the inputs using the OpenVLA processor, visualize the raw RGB image, print the raw instruction and formatted prompt, print the size of the processed input tensors, and print the OpenVLA model outputs for each step.\n",
    "\n",
    "By running these sections sequentially, we can evaluate the whole process for one task from the LIBERO dataset, visualize the raw RGB image, print the instructions and prompts, show the input tensor sizes, and print the OpenVLA model outputs for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhaoyu/anaconda3/envs/VLA_CL/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from easydict import EasyDict\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Load Processor and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Load LIBERO Demonstration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default benchmark root path:  /home/zhaoyu/Workspace/VLA-Continual-Learning/external/LIBERO/libero/libero\n",
      "Default dataset root path:  /data2/zhaoyu/LIBERO_dataset/datasets\n",
      "Dataset path: /data2/zhaoyu/LIBERO_dataset/datasets/libero_spatial\n"
     ]
    }
   ],
   "source": [
    "# Check dataset path\n",
    "BENCHMARK_PATH = get_libero_path(\"benchmark_root\")\n",
    "DATASET_BASE_PATH = get_libero_path(\"datasets\")\n",
    "print(\"Default benchmark root path: \", BENCHMARK_PATH)\n",
    "print(\"Default dataset root path: \", DATASET_BASE_PATH)\n",
    "\n",
    "# Select a dataset\n",
    "DATASET_NAME = \"libero_spatial\"\n",
    "FILTER_KEY = None  # Set filter key if needed, e.g., \"valid\" for validation\n",
    "VERBOSE = True\n",
    "dataset_path_demo = os.path.join(DATASET_BASE_PATH, DATASET_NAME)\n",
    "print(f\"Dataset path: {dataset_path_demo}\")\n",
    "\n",
    "# Load dataset\n",
    "# use a dictionary to store demonstration data for each task\n",
    "demonstration_data = {}\n",
    "# get all task names in the dataset\n",
    "task_names_demo = get_task_names(dataset_path_demo)\n",
    "# get demonstration data for each task\n",
    "\n",
    "# for task_name_demo in task_names_demo:\n",
    "#     print(f\"Loading demonstration data for task: {task_name_demo}\")\n",
    "#     [language_instruction, actions_batch, images_batch] = extract_task_info(dataset_path_demo, task_name, filter_key=FILTER_KEY, verbose=VERBOSE)\n",
    "#     demonstration_data[task_name_demo] = [language_instruction, actions_batch, images_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Train OpenVLA on LIBERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Evaluate OpenVLA on Trained LIBERO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUJOCO_GL=osmesa\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Task names for evaluation: ['pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate', 'pick_up_the_black_bowl_next_to_the_ramekin_and_place_it_on_the_plate', 'pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate', 'pick_up_the_black_bowl_on_the_cookie_box_and_place_it_on_the_plate', 'pick_up_the_black_bowl_in_the_top_drawer_of_the_wooden_cabinet_and_place_it_on_the_plate', 'pick_up_the_black_bowl_on_the_ramekin_and_place_it_on_the_plate', 'pick_up_the_black_bowl_next_to_the_cookie_box_and_place_it_on_the_plate', 'pick_up_the_black_bowl_on_the_stove_and_place_it_on_the_plate', 'pick_up_the_black_bowl_next_to_the_plate_and_place_it_on_the_plate', 'pick_up_the_black_bowl_on_the_wooden_cabinet_and_place_it_on_the_plate']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------init offscreen--------\n",
      "-------init 1--------\n",
      "-------init 2--------\n",
      "-------init 3--------\n",
      "-------init 4--------\n",
      "-------init--------\n",
      "-------init mujoco--------\n",
      "CREATED CON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhaoyu/anaconda3/envs/VLA_CL/lib/python3.8/site-packages/numba/np/arraymath.py:3845: DeprecationWarning: `np.MachAr` is deprecated (NumPy 1.22).\n",
      "  @overload(np.MachAr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------init offscreen--------\n",
      "-------init 1--------\n",
      "-------init 2--------\n",
      "-------init 3--------\n",
      "-------init 4--------\n",
      "-------init--------\n",
      "-------init mujoco--------\n",
      "CREATED CON\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m     steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     68\u001b[0m     obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mset_init_state(init_states_)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     num_success = 0\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#     for _ in range(5):  # simulate the physics without any actions\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#         env.step(np.zeros((env_num, 7)))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# print(f\"Results are saved at {save_folder}\")\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(test_loss, success_rate)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "%env MUJOCO_GL=osmesa\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "from libero.lifelong.metric import (\n",
    "    evaluate_loss,\n",
    "    evaluate_success,\n",
    "    raw_obs_to_tensor_obs,\n",
    ")\n",
    "from libero.lifelong.utils import (\n",
    "    control_seed,\n",
    "    safe_device,\n",
    "    torch_load_model,\n",
    "    NpEncoder,\n",
    "    compute_flops,\n",
    ")\n",
    "from libero.libero.envs import OffScreenRenderEnv, SubprocVectorEnv\n",
    "\n",
    "\n",
    "BDDL_FILES_BASE_PATH = get_libero_path(\"bddl_files\")\n",
    "INIT_STATES_BASE_PATH = get_libero_path(\"init_states\")\n",
    "VIDEO_FOLDER = \"../videos\"\n",
    "SAVE_VIDEO = False\n",
    "\n",
    "EVAL_MAX_STEP = 500\n",
    "\n",
    "# Load evaluation dataset\n",
    "# task_names_demo = list(demonstration_data.keys())\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "benchmark_instance = benchmark_dict[DATASET_NAME]()\n",
    "# num_tasks_eval = benchmark_instance.get_num_tasks()\n",
    "task_names_eval = benchmark_instance.get_task_names()\n",
    "print(f\"Task names for evaluation: {task_names_eval}\")\n",
    "# print(f\"Task name for training: {task_names_demo}\")\n",
    "\n",
    "# Evaluate the model\n",
    "task_id = 0\n",
    "task = benchmark_instance.get_task(task_id)\n",
    "\n",
    "with Timer() as t, VideoWriter(VIDEO_FOLDER, SAVE_VIDEO) as video_writer:\n",
    "    env_args = {\n",
    "        \"bddl_file_name\": os.path.join(\n",
    "            BDDL_FILES_BASE_PATH, task.problem_folder, task.bddl_file\n",
    "        ),\n",
    "        \"render_gpu_device_id\": 2\n",
    "    }\n",
    "\n",
    "    env_num = 1\n",
    "    env = SubprocVectorEnv(\n",
    "        [lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)]\n",
    "    )\n",
    "        \n",
    "    env.reset()\n",
    "    env.seed(0)\n",
    "\n",
    "    init_states_path = os.path.join(\n",
    "        INIT_STATES_BASE_PATH, task.problem_folder, task.init_states_file\n",
    "    )\n",
    "    init_states = torch.load(init_states_path)\n",
    "    indices = np.arange(env_num) % init_states.shape[0]\n",
    "    init_states_ = init_states[indices]\n",
    "\n",
    "    dones = [False] * env_num\n",
    "    steps = 0\n",
    "    obs = env.set_init_state(init_states_)\n",
    "    print(obs)\n",
    "\n",
    "#     num_success = 0\n",
    "#     for _ in range(5):  # simulate the physics without any actions\n",
    "#         env.step(np.zeros((env_num, 7)))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         while steps < EVAL_MAX_STEP:\n",
    "#             steps += 1\n",
    "\n",
    "#             # get current observation\n",
    "            \n",
    "            \n",
    "#             data = raw_obs_to_tensor_obs(obs, task_emb, cfg)\n",
    "#             actions = algo.policy.get_action(data)\n",
    "#             obs, reward, done, info = env.step(actions)\n",
    "#             video_writer.append_vector_obs(\n",
    "#                 obs, dones, camera_name=\"agentview_image\"\n",
    "#             )\n",
    "\n",
    "#             # check whether succeed\n",
    "#             for k in range(env_num):\n",
    "#                 dones[k] = dones[k] or done[k]\n",
    "#             if all(dones):\n",
    "#                 break\n",
    "\n",
    "#         for k in range(env_num):\n",
    "#             num_success += int(dones[k])\n",
    "\n",
    "#     success_rate = num_success / env_num\n",
    "#     env.close()\n",
    "\n",
    "#     eval_stats = {\n",
    "#         \"loss\": test_loss,\n",
    "#         \"success_rate\": success_rate,\n",
    "#     }\n",
    "\n",
    "#     os.system(f\"mkdir -p {args.save_dir}\")\n",
    "#     torch.save(eval_stats, save_folder)\n",
    "# print(\n",
    "#     f\"[info] finish for ckpt at {run_folder} in {t.get_elapsed_time()} sec for rollouts\"\n",
    "# )\n",
    "# print(f\"Results are saved at {save_folder}\")\n",
    "# print(test_loss, success_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla_cl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune OpenVLA with huggingface parameter efficient tuning method on LIBERO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LIBERO demonstration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset structure:\n",
    "language_instruction: a string of language instruction for the task\n",
    "actions_batch: numpy array with size: (50, N, 8)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of actions in each demonstration\n",
    "    - 8: action dimension\n",
    "images_batch: numpy array with size: (50, N, 128, 128, 3)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of images in each demonstration\n",
    "    - 128x128: image size\n",
    "    - 3: RGB\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "# currently no need to change FILTER_KEY and VERBOSE\n",
    "FILTER_KEY = None  # Set filter key if needed, e.g., \"valid\" for validation\n",
    "VERBOSE = True\n",
    "\n",
    "## Check libero dataset path\n",
    "BENCHMARK_PATH = get_libero_path(\"benchmark_root\")\n",
    "DATASET_BASE_PATH = get_libero_path(\"datasets\")\n",
    "DATASET_PATH_DEMO = os.path.join(DATASET_BASE_PATH, DATASET_NAME)\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO benchmark root path: \", BENCHMARK_PATH)\n",
    "print(\"LIBERO dataset root path: \", DATASET_BASE_PATH)\n",
    "print(f\"LIBERO demonstration dataset for {DATASET_NAME} path: {DATASET_PATH_DEMO}\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "## Load demonstration dataset\n",
    "# get all task names in the dataset\n",
    "task_names_demo = get_task_names(DATASET_PATH_DEMO)\n",
    "# print(f\"Tasks in the demonstration dataset: {task_names_demo}\")\n",
    "# load demonstration data for each task\n",
    "dataset_demo = {}\n",
    "print(\"Start loading demonstration data for each task...\")\n",
    "print(\"-------------------------------------\")\n",
    "for task_name_demo in task_names_demo:\n",
    "    print(f\"Loading demonstration data for task:\\n {task_name_demo}\")\n",
    "    [language_instruction, actions_batch, images_batch] = extract_task_info(DATASET_PATH_DEMO, task_name_demo, filter_key=FILTER_KEY, verbose=VERBOSE)\n",
    "    vla_prompt = f\"In: What action should the robot take to {language_instruction}?\\nOut:\"\n",
    "    dataset_demo[task_name_demo] = [vla_prompt, actions_batch, images_batch]\n",
    "    # check if actions_batch and images_batch have the same length\n",
    "    assert actions_batch.shape[0] == images_batch.shape[0], \"Dataset problem: the number of actions and images should be the same!\"\n",
    "    # print dataset information\n",
    "    print(\"Loaded successfully!\")\n",
    "    \n",
    "    print(f\"Total demonstrations: {actions_batch.shape[0]}\")\n",
    "    ave_len = np.mean([len(x) for x in actions_batch]) # average length of demonstrations\n",
    "    print(f\"Average demonstration length: {ave_len}\")\n",
    "    action_shape = actions_batch[0][0].shape # action shape\n",
    "    print(f\"Action shape: {action_shape}\")\n",
    "    img_shape = images_batch[0][0].shape # image shape\n",
    "    print(f\"Image shape: {img_shape}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to RLDS format (required by OpenVLA finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_SAVE_PATH = \"/data2/zhaoyu/LIBERO_rlds\"\n",
    "\n",
    "## Convert demonstration dataset to RLDS format\n",
    "episodes = []\n",
    "\n",
    "for task_name, (language_instruction, actions_batch, images_batch) in dataset_demo.items():\n",
    "    num_demos = actions_batch.shape[0]\n",
    "    \n",
    "    for i in range(num_demos):\n",
    "        episode = {\n",
    "            'language_instruction': language_instruction,\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        num_steps = actions_batch[i].shape[0]\n",
    "        for j in range(num_steps):\n",
    "            step = {\n",
    "                'observation': {\n",
    "                    'image': images_batch[i][j]\n",
    "                },\n",
    "                'action': actions_batch[i][j],  # action dimension is 7\n",
    "                'reward': 0.0,  # Update with actual reward if available\n",
    "                'is_last': (j == num_steps - 1)\n",
    "            }\n",
    "            episode['steps'].append(step)\n",
    "        \n",
    "        episodes.append(episode)\n",
    "\n",
    "## Save the dataset as a pickle file\n",
    "os.makedirs(DATASET_SAVE_PATH, exist_ok=True)\n",
    "output_path = os.path.join(DATASET_SAVE_PATH, f'{DATASET_NAME}.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(episodes, f)\n",
    "\n",
    "print(f\"Dataset converted and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "%env TOKENIZERS_PARALLELISM = false\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data2/zhaoyu/huggingface_cache'\n",
    "\n",
    "## Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/openvla')))\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "BASE_STORAGE_PATH = '/data2/zhaoyu/LIBERO_finetune'\n",
    "DATASET_SAVE_PATH = \"/data2/zhaoyu/LIBERO_rlds\"\n",
    "CHECKPOINT_PATH = os.path.join(BASE_STORAGE_PATH, f'checkpoints/{DATASET_NAME}')\n",
    "LOGS_PATH = os.path.join(BASE_STORAGE_PATH, f'logs/{DATASET_NAME}')\n",
    "RLDS_DATASET_PATH = os.path.join(DATASET_SAVE_PATH, f'{DATASET_NAME}.pkl')\n",
    "# create directories if they do not exist\n",
    "os.makedirs(DATASET_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_PATH, exist_ok=True)\n",
    "# print confirmation of environment setup\n",
    "print(\"Environment setup complete.\")\n",
    "# print(f\"TRANSFORMERS_CACHE set to: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "print(f\"RLDS Dataset path: {DATASET_SAVE_PATH}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Logs path: {LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "\n",
    "## Finetune configuration\n",
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # Training hyperparameters\n",
    "    batch_size: int = 12  # Batch size for training, adjust based on GPU memory\n",
    "    epochs: int = 10  # Number of training epochs\n",
    "    learning_rate: float = 5e-5  # Learning rate for the optimizer\n",
    "    lora_rank: int = 32  # LoRA rank for low-rank adaptation\n",
    "    target_modules: str = \"all-linear\"  # Target modules for LoRA\n",
    "\n",
    "    # Distributed training settings\n",
    "    world_size: int = torch.cuda.device_count()  # Number of GPUs available for training\n",
    "    rank: int = int(os.getenv('RANK', 0))  # Rank of the current process\n",
    "    local_rank: int = int(os.getenv('LOCAL_RANK', 0))  # Local rank of the current process\n",
    "\n",
    "    # Logging configurations\n",
    "    wandb_project: str = \"OpenVLA_Finetuning\"  # Project name for Weights & Biases logging\n",
    "    wandb_run_name: str = f\"finetune_{DATASET_NAME}\"  # Run name for Weights & Biases logging\n",
    "\n",
    "# Initialize the configuration\n",
    "finetune_config = FinetuneConfig()\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"Configuration parameters set:\")\n",
    "print(f\"Batch size: {finetune_config.batch_size}\")\n",
    "print(f\"Epochs: {finetune_config.epochs}\")\n",
    "print(f\"Learning rate: {finetune_config.learning_rate}\")\n",
    "print(f\"LoRA rank: {finetune_config.lora_rank}\")\n",
    "print(f\"Target modules: {finetune_config.target_modules}\")\n",
    "print(f\"World size (number of GPUs): {finetune_config.world_size}\")\n",
    "print(f\"Rank: {finetune_config.rank}\")\n",
    "print(f\"Local rank: {finetune_config.local_rank}\")\n",
    "print(f\"WANDB project: {finetune_config.wandb_project}\")\n",
    "print(f\"WANDB run name: {finetune_config.wandb_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## Custom dataset to handle RLDS formatted LIBERO data\n",
    "class RLDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        episode = self.data[idx]\n",
    "        language_instruction = episode['language_instruction']\n",
    "        steps = episode['steps']\n",
    "        return language_instruction, steps\n",
    "    \n",
    "## Load the RLDS formatted LIBERO dataset\n",
    "dataset_path = RLDS_DATASET_PATH\n",
    "print(f\"Loading dataset from {dataset_path}\")\n",
    "rlds_dataset = RLDataset(dataset_path)\n",
    "print(f\"Loaded {len(rlds_dataset)} episodes\")\n",
    "\n",
    "## Collator function to prepare batches\n",
    "def collate_fn(batch):\n",
    "    language_instructions = [item[0] for item in batch]\n",
    "    steps = [item[1] for item in batch]\n",
    "    \n",
    "    # Ensure the number of language instructions matches the number of episodes\n",
    "    assert len(language_instructions) == len(steps), \"Mismatch between language instructions and steps\"\n",
    "\n",
    "    # Extract images, actions, rewards, and is_last flags\n",
    "    images = [step['observation']['image'] for episode in steps for step in episode]\n",
    "    actions = [step['action'] for episode in steps for step in episode]\n",
    "    rewards = [step['reward'] for episode in steps for step in episode]\n",
    "    is_last = [step['is_last'] for episode in steps for step in episode]\n",
    "\n",
    "    # Convert lists of numpy arrays to single numpy arrays before converting to tensors\n",
    "    images = np.array(images)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    is_last = np.array(is_last)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    is_last = torch.tensor(is_last, dtype=torch.bool)\n",
    "\n",
    "    return {\n",
    "        'language_instructions': language_instructions,\n",
    "        'images': images,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards,\n",
    "        'is_last': is_last\n",
    "    }\n",
    "\n",
    "# Test the updated collate function with DataLoader\n",
    "data_loader = DataLoader(\n",
    "    rlds_dataset,\n",
    "    batch_size=finetune_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Print a batch to verify\n",
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "del(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Set the path to the pre-trained model cache\n",
    "PRETRAINED_MODEL_PATH = \"openvla/openvla-7b\"\n",
    "\n",
    "# Load the pre-trained OpenVLA model and processor\n",
    "print(f\"Loading pre-trained model from {PRETRAINED_MODEL_PATH}\")\n",
    "processor = AutoProcessor.from_pretrained(PRETRAINED_MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    PRETRAINED_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Configure LoRA for low-rank adaptation based on the example script\n",
    "lora_config = LoraConfig(\n",
    "    r=finetune_config.lora_rank,\n",
    "    lora_alpha=min(finetune_config.lora_rank, 16),\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=finetune_config.target_modules,\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Move the model to the correct device and use DataParallel for multi-GPU training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model preparation complete.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA_CL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune OpenVLA with huggingface parameter efficient tuning method on LIBERO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LIBERO demonstration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset structure:\n",
    "language_instruction: a string of language instruction for the task\n",
    "actions_batch: numpy array with size: (50, N, 8)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of actions in each demonstration\n",
    "    - 8: action dimension\n",
    "images_batch: numpy array with size: (50, N, 128, 128, 3)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of images in each demonstration\n",
    "    - 128x128: image size\n",
    "    - 3: RGB\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "# currently no need to change FILTER_KEY and VERBOSE\n",
    "FILTER_KEY = None  # Set filter key if needed, e.g., \"valid\" for validation\n",
    "VERBOSE = True\n",
    "\n",
    "## Check libero dataset path\n",
    "BENCHMARK_PATH = get_libero_path(\"benchmark_root\")\n",
    "DATASET_BASE_PATH = get_libero_path(\"datasets\")\n",
    "DATASET_PATH_DEMO = os.path.join(DATASET_BASE_PATH, DATASET_NAME)\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO benchmark root path: \", BENCHMARK_PATH)\n",
    "print(\"LIBERO dataset root path: \", DATASET_BASE_PATH)\n",
    "print(f\"LIBERO demonstration dataset for {DATASET_NAME} path: {DATASET_PATH_DEMO}\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "## Load demonstration dataset\n",
    "# get all task names in the dataset\n",
    "task_names_demo = get_task_names(DATASET_PATH_DEMO)\n",
    "# print(f\"Tasks in the demonstration dataset: {task_names_demo}\")\n",
    "# load demonstration data for each task\n",
    "dataset_demo = {}\n",
    "print(\"Start loading demonstration data for each task...\")\n",
    "print(\"-------------------------------------\")\n",
    "for task_name_demo in task_names_demo:\n",
    "    print(f\"Loading demonstration data for task:\\n {task_name_demo}\")\n",
    "    [language_instruction, actions_batch, images_batch] = extract_task_info(DATASET_PATH_DEMO, task_name_demo, filter_key=FILTER_KEY, verbose=VERBOSE)\n",
    "    dataset_demo[task_name_demo] = [language_instruction, actions_batch, images_batch]\n",
    "    # check if actions_batch and images_batch have the same length\n",
    "    assert actions_batch.shape[0] == images_batch.shape[0], \"Dataset problem: the number of actions and images should be the same!\"\n",
    "    # print dataset information\n",
    "    print(\"Loaded successfully!\")\n",
    "    \n",
    "    print(f\"Total demonstrations: {actions_batch.shape[0]}\")\n",
    "    ave_len = np.mean([len(x) for x in actions_batch]) # average length of demonstrations\n",
    "    print(f\"Average demonstration length: {ave_len}\")\n",
    "    action_shape = actions_batch[0][0].shape # action shape\n",
    "    print(f\"Action shape: {action_shape}\")\n",
    "    img_shape = images_batch[0][0].shape # image shape\n",
    "    print(f\"Image shape: {img_shape}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to RLDS format (required by OpenVLA finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_SAVE_PATH = \"/data2/zhaoyu/LIBERO_rlds\"\n",
    "\n",
    "## Convert demonstration dataset to RLDS format\n",
    "episodes = []\n",
    "\n",
    "for task_name, (language_instruction, actions_batch, images_batch) in dataset_demo.items():\n",
    "    num_demos = actions_batch.shape[0]\n",
    "    \n",
    "    for i in range(num_demos):\n",
    "        episode = {\n",
    "            'language_instruction': language_instruction,\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        num_steps = actions_batch[i].shape[0]\n",
    "        for j in range(num_steps):\n",
    "            step = {\n",
    "                'observation': {\n",
    "                    'image': images_batch[i][j]\n",
    "                },\n",
    "                'action': actions_batch[i][j],  # action dimension is 7\n",
    "                'reward': 0.0,  # Update with actual reward if available\n",
    "                'is_last': (j == num_steps - 1)\n",
    "            }\n",
    "            episode['steps'].append(step)\n",
    "        \n",
    "        episodes.append(episode)\n",
    "\n",
    "## Save the dataset as a pickle file\n",
    "os.makedirs(DATASET_SAVE_PATH, exist_ok=True)\n",
    "output_path = os.path.join(DATASET_SAVE_PATH, f'{DATASET_NAME}.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(episodes, f)\n",
    "\n",
    "print(f\"Dataset converted and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
      "env: TOKENIZERS_PARALLELISM=false\n",
      "Environment setup complete.\n",
      "RLDS Dataset path: /data2/zhaoyu/LIBERO_rlds\n",
      "Checkpoint path: /data2/zhaoyu/LIBERO_finetune/checkpoints/libero_spatial\n",
      "Logs path: /data2/zhaoyu/LIBERO_finetune/logs/libero_spatial\n"
     ]
    }
   ],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "%env TOKENIZERS_PARALLELISM = false\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data2/zhaoyu/huggingface_cache'\n",
    "\n",
    "## Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/openvla')))\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "BASE_STORAGE_PATH = '/data2/zhaoyu/LIBERO_finetune'\n",
    "DATASET_SAVE_PATH = \"/data2/zhaoyu/LIBERO_rlds\"\n",
    "CHECKPOINT_PATH = os.path.join(BASE_STORAGE_PATH, f'checkpoints/{DATASET_NAME}')\n",
    "LOGS_PATH = os.path.join(BASE_STORAGE_PATH, f'logs/{DATASET_NAME}')\n",
    "RLDS_DATASET_PATH = os.path.join(DATASET_SAVE_PATH, f'{DATASET_NAME}.pkl')\n",
    "# create directories if they do not exist\n",
    "os.makedirs(DATASET_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_PATH, exist_ok=True)\n",
    "# print confirmation of environment setup\n",
    "print(\"Environment setup complete.\")\n",
    "# print(f\"TRANSFORMERS_CACHE set to: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "print(f\"RLDS Dataset path: {DATASET_SAVE_PATH}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Logs path: {LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters set:\n",
      "Batch size: 12\n",
      "Epochs: 10\n",
      "Learning rate: 5e-05\n",
      "LoRA rank: 32\n",
      "Target modules: all-linear\n",
      "World size (number of GPUs): 8\n",
      "Rank: 0\n",
      "Local rank: 0\n",
      "WANDB project: OpenVLA_Finetuning\n",
      "WANDB run name: finetune_libero_spatial\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "\n",
    "## Finetune configuration\n",
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # Training hyperparameters\n",
    "    batch_size: int = 12  # Batch size for training, adjust based on GPU memory\n",
    "    epochs: int = 10  # Number of training epochs\n",
    "    learning_rate: float = 5e-5  # Learning rate for the optimizer\n",
    "    lora_rank: int = 32  # LoRA rank for low-rank adaptation\n",
    "    target_modules: str = \"all-linear\"  # Target modules for LoRA\n",
    "\n",
    "    # Distributed training settings\n",
    "    world_size: int = torch.cuda.device_count()  # Number of GPUs available for training\n",
    "    rank: int = int(os.getenv('RANK', 0))  # Rank of the current process\n",
    "    local_rank: int = int(os.getenv('LOCAL_RANK', 0))  # Local rank of the current process\n",
    "\n",
    "    # Logging configurations\n",
    "    wandb_project: str = \"OpenVLA_Finetuning\"  # Project name for Weights & Biases logging\n",
    "    wandb_run_name: str = f\"finetune_{DATASET_NAME}\"  # Run name for Weights & Biases logging\n",
    "\n",
    "# Initialize the configuration\n",
    "finetune_config = FinetuneConfig()\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"Configuration parameters set:\")\n",
    "print(f\"Batch size: {finetune_config.batch_size}\")\n",
    "print(f\"Epochs: {finetune_config.epochs}\")\n",
    "print(f\"Learning rate: {finetune_config.learning_rate}\")\n",
    "print(f\"LoRA rank: {finetune_config.lora_rank}\")\n",
    "print(f\"Target modules: {finetune_config.target_modules}\")\n",
    "print(f\"World size (number of GPUs): {finetune_config.world_size}\")\n",
    "print(f\"Rank: {finetune_config.rank}\")\n",
    "print(f\"Local rank: {finetune_config.local_rank}\")\n",
    "print(f\"WANDB project: {finetune_config.wandb_project}\")\n",
    "print(f\"WANDB run name: {finetune_config.wandb_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /data2/zhaoyu/LIBERO_rlds/libero_spatial.pkl\n",
      "Loaded 500 episodes\n",
      "{'language_instructions': ['pick up the black bowl on the wooden cabinet and place it on the plate', 'pick up the black bowl from table center and place it on the plate', 'pick up the black bowl next to the ramekin and place it on the plate', 'pick up the black bowl next to the plate and place it on the plate', 'pick up the black bowl next to the plate and place it on the plate', 'pick up the black bowl on the wooden cabinet and place it on the plate', 'pick up the black bowl on the stove and place it on the plate', 'pick up the black bowl next to the ramekin and place it on the plate', 'pick up the black bowl on the wooden cabinet and place it on the plate', 'pick up the black bowl on the ramekin and place it on the plate', 'pick up the black bowl on the stove and place it on the plate', 'pick up the black bowl between the plate and the ramekin and place it on the plate'], 'images': tensor([[[[109., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 104.],\n",
      "          [110., 108., 104.],\n",
      "          [110., 107., 104.]],\n",
      "\n",
      "         [[108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 108., 104.]],\n",
      "\n",
      "         [[109., 107., 104.],\n",
      "          [108., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 107., 104.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[177., 161., 143.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 162., 144.],\n",
      "          ...,\n",
      "          [200., 184., 168.],\n",
      "          [196., 179., 163.],\n",
      "          [198., 181., 165.]],\n",
      "\n",
      "         [[178., 163., 144.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 163., 145.],\n",
      "          ...,\n",
      "          [202., 185., 169.],\n",
      "          [197., 181., 164.],\n",
      "          [197., 180., 164.]],\n",
      "\n",
      "         [[180., 165., 146.],\n",
      "          [180., 164., 145.],\n",
      "          [181., 165., 146.],\n",
      "          ...,\n",
      "          [200., 183., 167.],\n",
      "          [199., 182., 166.],\n",
      "          [196., 180., 163.]]],\n",
      "\n",
      "\n",
      "        [[[109., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 104.],\n",
      "          [110., 108., 104.],\n",
      "          [110., 107., 104.]],\n",
      "\n",
      "         [[108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 108., 104.]],\n",
      "\n",
      "         [[109., 107., 104.],\n",
      "          [108., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 107., 104.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[177., 161., 143.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 162., 144.],\n",
      "          ...,\n",
      "          [200., 184., 168.],\n",
      "          [196., 179., 163.],\n",
      "          [198., 181., 165.]],\n",
      "\n",
      "         [[178., 163., 144.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 163., 145.],\n",
      "          ...,\n",
      "          [202., 185., 169.],\n",
      "          [197., 181., 164.],\n",
      "          [197., 180., 164.]],\n",
      "\n",
      "         [[180., 165., 146.],\n",
      "          [180., 164., 145.],\n",
      "          [181., 165., 146.],\n",
      "          ...,\n",
      "          [200., 183., 167.],\n",
      "          [199., 182., 166.],\n",
      "          [196., 180., 163.]]],\n",
      "\n",
      "\n",
      "        [[[109., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 104.],\n",
      "          [110., 108., 104.],\n",
      "          [110., 107., 104.]],\n",
      "\n",
      "         [[108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 108., 104.]],\n",
      "\n",
      "         [[109., 107., 104.],\n",
      "          [108., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 107., 104.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[177., 161., 143.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 162., 144.],\n",
      "          ...,\n",
      "          [200., 184., 168.],\n",
      "          [196., 179., 163.],\n",
      "          [198., 181., 165.]],\n",
      "\n",
      "         [[178., 163., 144.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 163., 145.],\n",
      "          ...,\n",
      "          [202., 185., 169.],\n",
      "          [197., 181., 164.],\n",
      "          [197., 180., 164.]],\n",
      "\n",
      "         [[180., 165., 146.],\n",
      "          [180., 164., 145.],\n",
      "          [181., 165., 146.],\n",
      "          ...,\n",
      "          [200., 183., 167.],\n",
      "          [199., 182., 166.],\n",
      "          [196., 180., 163.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[109., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 104.],\n",
      "          [110., 108., 104.],\n",
      "          [110., 107., 104.]],\n",
      "\n",
      "         [[108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 108., 104.]],\n",
      "\n",
      "         [[109., 107., 104.],\n",
      "          [108., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 107., 104.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[177., 161., 143.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 162., 144.],\n",
      "          ...,\n",
      "          [200., 184., 168.],\n",
      "          [196., 179., 163.],\n",
      "          [198., 181., 165.]],\n",
      "\n",
      "         [[178., 163., 144.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 163., 145.],\n",
      "          ...,\n",
      "          [202., 185., 169.],\n",
      "          [197., 181., 164.],\n",
      "          [197., 180., 164.]],\n",
      "\n",
      "         [[180., 165., 146.],\n",
      "          [180., 164., 145.],\n",
      "          [181., 165., 146.],\n",
      "          ...,\n",
      "          [200., 183., 167.],\n",
      "          [199., 182., 166.],\n",
      "          [196., 180., 163.]]],\n",
      "\n",
      "\n",
      "        [[[109., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 104.],\n",
      "          [110., 108., 104.],\n",
      "          [110., 107., 104.]],\n",
      "\n",
      "         [[108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 108., 104.]],\n",
      "\n",
      "         [[109., 107., 104.],\n",
      "          [108., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 107., 104.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[177., 161., 143.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 162., 144.],\n",
      "          ...,\n",
      "          [200., 184., 168.],\n",
      "          [196., 179., 163.],\n",
      "          [198., 181., 165.]],\n",
      "\n",
      "         [[178., 163., 144.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 163., 145.],\n",
      "          ...,\n",
      "          [202., 185., 169.],\n",
      "          [197., 181., 164.],\n",
      "          [197., 180., 164.]],\n",
      "\n",
      "         [[180., 165., 146.],\n",
      "          [180., 164., 145.],\n",
      "          [181., 165., 146.],\n",
      "          ...,\n",
      "          [200., 183., 167.],\n",
      "          [199., 182., 166.],\n",
      "          [196., 180., 163.]]],\n",
      "\n",
      "\n",
      "        [[[109., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 104.],\n",
      "          [110., 108., 104.],\n",
      "          [110., 107., 104.]],\n",
      "\n",
      "         [[108., 106., 103.],\n",
      "          [109., 107., 103.],\n",
      "          [109., 107., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 108., 104.]],\n",
      "\n",
      "         [[109., 107., 104.],\n",
      "          [108., 107., 103.],\n",
      "          [108., 106., 103.],\n",
      "          ...,\n",
      "          [110., 108., 105.],\n",
      "          [110., 108., 105.],\n",
      "          [109., 107., 104.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[177., 161., 143.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 162., 144.],\n",
      "          ...,\n",
      "          [200., 184., 168.],\n",
      "          [196., 179., 163.],\n",
      "          [198., 181., 165.]],\n",
      "\n",
      "         [[178., 163., 144.],\n",
      "          [179., 163., 145.],\n",
      "          [179., 163., 145.],\n",
      "          ...,\n",
      "          [202., 185., 169.],\n",
      "          [197., 181., 164.],\n",
      "          [197., 180., 164.]],\n",
      "\n",
      "         [[180., 165., 146.],\n",
      "          [180., 164., 145.],\n",
      "          [181., 165., 146.],\n",
      "          ...,\n",
      "          [200., 183., 167.],\n",
      "          [199., 182., 166.],\n",
      "          [196., 180., 163.]]]]), 'actions': tensor([[ 0.2009, -0.1527,  0.3402,  ...,  0.0086, -0.0000, -1.0000],\n",
      "        [ 0.3830, -0.2411,  0.4473,  ...,  0.0043, -0.0000, -1.0000],\n",
      "        [ 0.4420, -0.3589,  0.6134,  ...,  0.0000, -0.0000, -1.0000],\n",
      "        ...,\n",
      "        [ 0.0750, -0.0750,  0.9375,  ...,  0.0954, -0.0000, -1.0000],\n",
      "        [ 0.0643, -0.0911,  0.9375,  ...,  0.1039, -0.0000, -1.0000],\n",
      "        [ 0.0536, -0.0911,  0.9214,  ...,  0.1093, -0.0000, -1.0000]]), 'rewards': tensor([0., 0., 0.,  ..., 0., 0., 0.]), 'is_last': tensor([False, False, False,  ..., False, False,  True])}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## Custom dataset to handle RLDS formatted LIBERO data\n",
    "class RLDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        episode = self.data[idx]\n",
    "        language_instruction = episode['language_instruction']\n",
    "        steps = episode['steps']\n",
    "        return language_instruction, steps\n",
    "    \n",
    "## Load the RLDS formatted LIBERO dataset\n",
    "dataset_path = RLDS_DATASET_PATH\n",
    "print(f\"Loading dataset from {dataset_path}\")\n",
    "rlds_dataset = RLDataset(dataset_path)\n",
    "print(f\"Loaded {len(rlds_dataset)} episodes\")\n",
    "\n",
    "## Collator function to prepare batches\n",
    "def collate_fn(batch):\n",
    "    language_instructions = [item[0] for item in batch]\n",
    "    steps = [item[1] for item in batch]\n",
    "    \n",
    "    # Ensure the number of language instructions matches the number of episodes\n",
    "    assert len(language_instructions) == len(steps), \"Mismatch between language instructions and steps\"\n",
    "\n",
    "    # Extract images, actions, rewards, and is_last flags\n",
    "    images = [step['observation']['image'] for episode in steps for step in episode]\n",
    "    actions = [step['action'] for episode in steps for step in episode]\n",
    "    rewards = [step['reward'] for episode in steps for step in episode]\n",
    "    is_last = [step['is_last'] for episode in steps for step in episode]\n",
    "\n",
    "    # Convert lists of numpy arrays to single numpy arrays before converting to tensors\n",
    "    images = np.array(images)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    is_last = np.array(is_last)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    is_last = torch.tensor(is_last, dtype=torch.bool)\n",
    "\n",
    "    return {\n",
    "        'language_instructions': language_instructions,\n",
    "        'images': images,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards,\n",
    "        'is_last': is_last\n",
    "    }\n",
    "\n",
    "# Test the updated collate function with DataLoader\n",
    "data_loader = DataLoader(\n",
    "    rlds_dataset,\n",
    "    batch_size=finetune_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Print a batch to verify\n",
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
      "Loading pre-trained model from openvla/openvla-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 5430.69it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs for training\n",
      "Model preparation complete.\n",
      "DataParallel(\n",
      "  (module): PeftModel(\n",
      "    (base_model): LoraModel(\n",
      "      (model): OpenVLAForActionPrediction(\n",
      "        (vision_backbone): PrismaticVisionBackbone(\n",
      "          (featurizer): VisionTransformer(\n",
      "            (patch_embed): PatchEmbed(\n",
      "              (proj): lora.Conv2d(\n",
      "                (base_layer): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(3, 32, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(32, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (norm): Identity()\n",
      "            )\n",
      "            (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "            (patch_drop): Identity()\n",
      "            (norm_pre): Identity()\n",
      "            (blocks): Sequential(\n",
      "              (0): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (1): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (2): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (3): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (4): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (5): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (6): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (7): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (8): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (9): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (10): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (11): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (12): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (13): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (14): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (15): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (16): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (17): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (18): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (19): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (20): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (21): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (22): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (23): Block(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3072, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): LayerScale()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): LayerScale()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "            )\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (fc_norm): Identity()\n",
      "            (head_drop): Dropout(p=0.0, inplace=False)\n",
      "            (head): Identity()\n",
      "          )\n",
      "          (fused_featurizer): VisionTransformer(\n",
      "            (patch_embed): PatchEmbed(\n",
      "              (proj): lora.Conv2d(\n",
      "                (base_layer): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(3, 32, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(32, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (norm): Identity()\n",
      "            )\n",
      "            (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "            (patch_drop): Identity()\n",
      "            (norm_pre): Identity()\n",
      "            (blocks): Sequential(\n",
      "              (0): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (1): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (2): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (3): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (4): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (5): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (6): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (7): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (8): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (9): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (10): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (11): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (12): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (13): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (14): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (15): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (16): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (17): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (18): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (19): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (20): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (21): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (22): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (23): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (24): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (25): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "              (26): Block(\n",
      "                (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (attn): Attention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=3456, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (q_norm): Identity()\n",
      "                  (k_norm): Identity()\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls1): Identity()\n",
      "                (drop_path1): Identity()\n",
      "                (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (drop1): Dropout(p=0.0, inplace=False)\n",
      "                  (norm): Identity()\n",
      "                  (fc2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (drop2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (ls2): Identity()\n",
      "                (drop_path2): Identity()\n",
      "              )\n",
      "            )\n",
      "            (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn_pool): AttentionPoolLatent(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (kv): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1152, out_features=2304, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1152, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4304, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1152, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (fc_norm): Identity()\n",
      "            (head_drop): Dropout(p=0.0, inplace=False)\n",
      "            (head): Identity()\n",
      "          )\n",
      "        )\n",
      "        (projector): PrismaticProjector(\n",
      "          (fc1): lora.Linear(\n",
      "            (base_layer): Linear(in_features=2176, out_features=8704, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=2176, out_features=32, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=32, out_features=8704, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (fc2): lora.Linear(\n",
      "            (base_layer): Linear(in_features=8704, out_features=4096, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=8704, out_features=32, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (fc3): lora.Linear(\n",
      "            (base_layer): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (act_fn1): GELU(approximate='none')\n",
      "          (act_fn2): GELU(approximate='none')\n",
      "        )\n",
      "        (language_model): LlamaForCausalLM(\n",
      "          (model): LlamaModel(\n",
      "            (embed_tokens): Embedding(32064, 4096, padding_idx=32000)\n",
      "            (layers): ModuleList(\n",
      "              (0-31): 32 x LlamaDecoderLayer(\n",
      "                (self_attn): LlamaSdpaAttention(\n",
      "                  (q_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (rotary_emb): LlamaRotaryEmbedding()\n",
      "                )\n",
      "                (mlp): LlamaMLP(\n",
      "                  (gate_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=11008, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (up_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=11008, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (down_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=11008, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "                (input_layernorm): LlamaRMSNorm()\n",
      "                (post_attention_layernorm): LlamaRMSNorm()\n",
      "              )\n",
      "            )\n",
      "            (norm): LlamaRMSNorm()\n",
      "          )\n",
      "          (lm_head): lora.Linear(\n",
      "            (base_layer): Linear(in_features=4096, out_features=32064, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=32, out_features=32064, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Set the path to the pre-trained model cache\n",
    "PRETRAINED_MODEL_PATH = \"openvla/openvla-7b\"\n",
    "\n",
    "# Load the pre-trained OpenVLA model and processor\n",
    "print(f\"Loading pre-trained model from {PRETRAINED_MODEL_PATH}\")\n",
    "processor = AutoProcessor.from_pretrained(PRETRAINED_MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    PRETRAINED_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Configure LoRA for low-rank adaptation based on the example script\n",
    "lora_config = LoraConfig(\n",
    "    r=finetune_config.lora_rank,\n",
    "    lora_alpha=min(finetune_config.lora_rank, 16),\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=finetune_config.target_modules,\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Move the model to the correct device and use DataParallel for multi-GPU training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model preparation complete.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/zhaoyu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zhaoyu/Workspace/VLA-Continual-Learning/notebooks/wandb/run-20240723_010804-v6403fw3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/object814-national-university-of-singapore/OpenVLA_Finetuning/runs/v6403fw3\" target=\"_blank\">finetune_libero_spatial</a></strong> to <a href=\"https://wandb.ai/object814-national-university-of-singapore/OpenVLA_Finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/42 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinetune_config\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss})\n",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, scheduler, criterion, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m language_instructions \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_instructions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Tokenize language instructions and move to device\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_instructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(pixel_values\u001b[38;5;241m=\u001b[39mimages, input_ids\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mattention_mask)\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'images'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming we already have the following imports based on your setup\n",
    "# from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Initialize Weights & Biases (wandb) for logging\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=finetune_config.wandb_project, name=finetune_config.wandb_run_name)\n",
    "\n",
    "# Initialize the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=finetune_config.learning_rate)\n",
    "num_training_steps = len(train_data_loader) * finetune_config.epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, data_loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        # Move data to the appropriate device\n",
    "        images = batch['images'].to(device)\n",
    "        actions = batch['actions'].to(device)\n",
    "        language_instructions = batch['language_instructions']\n",
    "        \n",
    "        # Tokenize language instructions and move to device\n",
    "        inputs = processor(text=language_instructions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=images, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "        loss = criterion(outputs.logits, actions)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Validation function\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validating\"):\n",
    "            # Move data to the appropriate device\n",
    "            images = batch['images'].to(device)\n",
    "            actions = batch['actions'].to(device)\n",
    "            language_instructions = batch['language_instructions']\n",
    "            \n",
    "            # Tokenize language instructions and move to device\n",
    "            inputs = processor(text=language_instructions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=images, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "            loss = criterion(outputs.logits, actions)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# DataLoader for training and validation (assuming validation DataLoader is available)\n",
    "train_data_loader = DataLoader(\n",
    "    rlds_dataset,\n",
    "    batch_size=finetune_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# For demonstration, assume validation_data_loader is similar to train_data_loader\n",
    "validation_data_loader = train_data_loader  # Replace with actual validation DataLoader if available\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(finetune_config.epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{finetune_config.epochs}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss = train(model, train_data_loader, optimizer, scheduler, criterion, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    wandb.log({\"train_loss\": train_loss})\n",
    "    \n",
    "    # Validate the model\n",
    "    validation_loss = validate(model, validation_data_loader, criterion, device)\n",
    "    print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "    wandb.log({\"validation_loss\": validation_loss})\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the final model\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_PATH, 'final_model.pth'))\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA_CL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

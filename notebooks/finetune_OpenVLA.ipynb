{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune OpenVLA with huggingface parameter efficient tuning method on LIBERO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LIBERO demonstration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "LIBERO benchmark root path:  /home/zhaoyu/Workspace/VLA-Continual-Learning/external/LIBERO/libero/libero\n",
      "LIBERO dataset root path:  /data2/zhaoyu/LIBERO_dataset/datasets\n",
      "LIBERO demonstration dataset for libero_10 path: /data2/zhaoyu/LIBERO_dataset/datasets/libero_10\n",
      "=====================================\n",
      "Start loading demonstration data for each task...\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " LIVING_ROOM_SCENE6_put_the_white_mug_on_the_plate_and_put_the_chocolate_pudding_to_the_right_of_the_plate_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 255.12\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " STUDY_SCENE1_pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 189.4\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " LIVING_ROOM_SCENE2_put_both_the_alphabet_soup_and_the_tomato_sauce_in_the_basket_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 294.0\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " KITCHEN_SCENE8_put_both_moka_pots_on_the_stove_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 415.88\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " KITCHEN_SCENE3_turn_on_the_stove_and_put_the_moka_pot_on_it_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 265.96\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " KITCHEN_SCENE6_put_the_yellow_and_white_mug_in_the_microwave_and_close_it_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 304.64\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " LIVING_ROOM_SCENE5_put_the_white_mug_on_the_left_plate_and_put_the_yellow_and_white_mug_on_the_right_plate_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 258.18\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " LIVING_ROOM_SCENE1_put_both_the_alphabet_soup_and_the_cream_cheese_box_in_the_basket_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 269.52\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " KITCHEN_SCENE4_put_the_black_bowl_in_the_bottom_drawer_of_the_cabinet_and_close_it_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 248.68\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n",
      "Loading demonstration data for task:\n",
      " LIVING_ROOM_SCENE2_put_both_the_cream_cheese_box_and_the_butter_in_the_basket_demo\n",
      "Loaded successfully!\n",
      "Total demonstrations: 50\n",
      "Average demonstration length: 260.42\n",
      "Action shape: (8,)\n",
      "Image shape: (128, 128, 3)\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataset structure:\n",
    "language_instruction: a string of language instruction for the task\n",
    "actions_batch: numpy array with size: (50, N, 8)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of actions in each demonstration\n",
    "    - 8: action dimension\n",
    "images_batch: numpy array with size: (50, N, 128, 128, 3)\n",
    "    - 50: number of demonstrations\n",
    "    - N: number of images in each demonstration\n",
    "    - 128x128: image size\n",
    "    - 3: RGB\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_10\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "# currently no need to change FILTER_KEY and VERBOSE\n",
    "FILTER_KEY = None  # Set filter key if needed, e.g., \"valid\" for validation\n",
    "VERBOSE = True\n",
    "\n",
    "## Check libero dataset path\n",
    "BENCHMARK_PATH = get_libero_path(\"benchmark_root\")\n",
    "DATASET_BASE_PATH = get_libero_path(\"datasets\")\n",
    "DATASET_PATH_DEMO = os.path.join(DATASET_BASE_PATH, DATASET_NAME)\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO benchmark root path: \", BENCHMARK_PATH)\n",
    "print(\"LIBERO dataset root path: \", DATASET_BASE_PATH)\n",
    "print(f\"LIBERO demonstration dataset for {DATASET_NAME} path: {DATASET_PATH_DEMO}\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "## Load demonstration dataset\n",
    "# get all task names in the dataset\n",
    "task_names_demo = get_task_names(DATASET_PATH_DEMO)\n",
    "# print(f\"Tasks in the demonstration dataset: {task_names_demo}\")\n",
    "# load demonstration data for each task\n",
    "dataset_demo = {}\n",
    "print(\"Start loading demonstration data for each task...\")\n",
    "print(\"-------------------------------------\")\n",
    "for task_name_demo in task_names_demo:\n",
    "    print(f\"Loading demonstration data for task:\\n {task_name_demo}\")\n",
    "    [language_instruction, actions_batch, images_batch] = extract_task_info(DATASET_PATH_DEMO, task_name_demo, filter_key=FILTER_KEY, verbose=VERBOSE)\n",
    "    dataset_demo[task_name_demo] = [language_instruction, actions_batch, images_batch]\n",
    "    # check if actions_batch and images_batch have the same length\n",
    "    assert actions_batch.shape[0] == images_batch.shape[0], \"Dataset problem: the number of actions and images should be the same!\"\n",
    "    # print dataset information\n",
    "    print(\"Loaded successfully!\")\n",
    "    print(f\"Total demonstrations: {actions_batch.shape[0]}\")\n",
    "    ave_len = np.mean([len(x) for x in actions_batch]) # average length of demonstrations\n",
    "    print(f\"Average demonstration length: {ave_len}\")\n",
    "    action_shape = actions_batch[0][0].shape # action shape\n",
    "    print(f\"Action shape: {action_shape}\")\n",
    "    img_shape = images_batch[0][0].shape # image shape\n",
    "    print(f\"Image shape: {img_shape}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to RLDS format (required by OpenVLA finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset converted and saved to /data2/zhaoyu/LIBERO_rlds/libero_10.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_SAVE_PATH = \"/data2/zhaoyu/LIBERO_rlds\"\n",
    "\n",
    "## Convert demonstration dataset to RLDS format\n",
    "episodes = []\n",
    "\n",
    "for task_name, (language_instruction, actions_batch, images_batch) in dataset_demo.items():\n",
    "    num_demos = actions_batch.shape[0]\n",
    "    \n",
    "    for i in range(num_demos):\n",
    "        episode = {\n",
    "            'language_instruction': language_instruction,\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        num_steps = actions_batch[i].shape[0]\n",
    "        for j in range(num_steps):\n",
    "            step = {\n",
    "                'observation': {\n",
    "                    'image': images_batch[i][j]\n",
    "                },\n",
    "                'action': actions_batch[i][j],  # action dimension is 7\n",
    "                'reward': 0.0,  # Update with actual reward if available\n",
    "                'is_last': (j == num_steps - 1)\n",
    "            }\n",
    "            episode['steps'].append(step)\n",
    "        \n",
    "        episodes.append(episode)\n",
    "\n",
    "## Save the dataset as a pickle file\n",
    "os.makedirs(DATASET_SAVE_PATH, exist_ok=True)\n",
    "output_path = os.path.join(DATASET_SAVE_PATH, f'{DATASET_NAME}.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(episodes, f)\n",
    "\n",
    "print(f\"Dataset converted and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhaoyu/anaconda3/envs/VLA_CL/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/zhaoyu/anaconda3/envs/VLA_CL/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prismatic.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibero\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibero\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m benchmark, get_libero_path\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLIBERO_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_task_names, extract_task_info\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PurePromptBuilder, VicunaV15ChatPromptBuilder\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaddedCollatorForActionPrediction\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maction_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActionTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prismatic.models'"
     ]
    }
   ],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "%env TOKENIZERS_PARALLELISM = false\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data2/zhaoyu/huggingface_cache'\n",
    "\n",
    "## Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "BASE_STORAGE_PATH = '/data2/zhaoyu/LIBERO_finetune'\n",
    "DATASET_SAVE_PATH = os.path.join(BASE_STORAGE_PATH, f'datasets/{DATASET_NAME}')\n",
    "CHECKPOINT_PATH = os.path.join(BASE_STORAGE_PATH, f'checkpoints/{DATASET_NAME}')\n",
    "LOGS_PATH = os.path.join(BASE_STORAGE_PATH, f'logs/{DATASET_NAME}')\n",
    "RLDS_DATASET_PATH = os.path.join(DATASET_SAVE_PATH, f'{DATASET_NAME}.pkl')\n",
    "# create directories if they do not exist\n",
    "os.makedirs(DATASET_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_PATH, exist_ok=True)\n",
    "# print confirmation of environment setup\n",
    "print(\"Environment setup complete.\")\n",
    "# print(f\"TRANSFORMERS_CACHE set to: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "print(f\"RLDS Dataset path: {DATASET_SAVE_PATH}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Logs path: {LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finetune configuration\n",
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # Training hyperparameters\n",
    "    batch_size: int = 12  # Batch size for training, adjust based on GPU memory\n",
    "    epochs: int = 10  # Number of training epochs\n",
    "    learning_rate: float = 5e-5  # Learning rate for the optimizer\n",
    "    lora_rank: int = 32  # LoRA rank for low-rank adaptation\n",
    "    target_modules: str = \"all-linear\"  # Target modules for LoRA\n",
    "\n",
    "    # Distributed training settings\n",
    "    world_size: int = torch.cuda.device_count()  # Number of GPUs available for training\n",
    "    rank: int = int(os.getenv('RANK', 0))  # Rank of the current process\n",
    "    local_rank: int = int(os.getenv('LOCAL_RANK', 0))  # Local rank of the current process\n",
    "\n",
    "    # Logging configurations\n",
    "    wandb_project: str = \"OpenVLA_Finetuning\"  # Project name for Weights & Biases logging\n",
    "    wandb_run_name: str = f\"finetune_{DATASET_NAME}\"  # Run name for Weights & Biases logging\n",
    "\n",
    "# Initialize the configuration\n",
    "finetune_config = FinetuneConfig()\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"Configuration parameters set:\")\n",
    "print(f\"Batch size: {finetune_config.batch_size}\")\n",
    "print(f\"Epochs: {finetune_config.epochs}\")\n",
    "print(f\"Learning rate: {finetune_config.learning_rate}\")\n",
    "print(f\"LoRA rank: {finetune_config.lora_rank}\")\n",
    "print(f\"Target modules: {finetune_config.target_modules}\")\n",
    "print(f\"World size (number of GPUs): {finetune_config.world_size}\")\n",
    "print(f\"Rank: {finetune_config.rank}\")\n",
    "print(f\"Local rank: {finetune_config.local_rank}\")\n",
    "print(f\"WANDB project: {finetune_config.wandb_project}\")\n",
    "print(f\"WANDB run name: {finetune_config.wandb_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## Custom dataset to handle RLDS formatted LIBERO data\n",
    "class RLDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        episode = self.data[idx]\n",
    "        language_instruction = episode['language_instruction']\n",
    "        steps = episode['steps']\n",
    "        return language_instruction, steps\n",
    "    \n",
    "## Load the RLDS formatted LIBERO dataset\n",
    "dataset_path = RLDS_DATASET_PATH\n",
    "print(f\"Loading dataset from {dataset_path}\")\n",
    "rlds_dataset = RLDataset(dataset_path)\n",
    "print(f\"Loaded {len(rlds_dataset)} episodes\")\n",
    "\n",
    "## Collator function to prepare batches\n",
    "def collate_fn(batch):\n",
    "    language_instructions = [item[0] for item in batch]\n",
    "    steps = [item[1] for item in batch]\n",
    "    images = [step['observation']['image'] for episode in steps for step in episode]\n",
    "    actions = [step['action'] for episode in steps for step in episode]\n",
    "    rewards = [step['reward'] for episode in steps for step in episode]\n",
    "    is_last = [step['is_last'] for episode in steps for step in episode]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    is_last = torch.tensor(is_last, dtype=torch.bool)\n",
    "\n",
    "    return {\n",
    "        'language_instructions': language_instructions,\n",
    "        'images': images,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards,\n",
    "        'is_last': is_last\n",
    "    }\n",
    "\n",
    "## Create DataLoader for the dataset\n",
    "data_loader = DataLoader(\n",
    "    rlds_dataset,\n",
    "    batch_size=finetune_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "## Print a batch to verify\n",
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA_CL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVLA evaluation on LIBERO benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Initialize OpenVLA with Huggingface transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data2/zhaoyu/huggingface_cache'\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import PeftModel, PeftConfig\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/openvla')))\n",
    "from prismatic.vla.datasets import RLDSBatchTransform\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "\n",
    "DEVICE_ID = 6 # GPU device id for rendering\n",
    "\n",
    "## Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "# base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "#     \"openvla/openvla-7b\",\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True\n",
    "# ).to(\"cuda:6\")\n",
    "\n",
    "checkpoint_path = '/data2/zhaoyu/LIBERO_finetune/checkpoints/libero_spatial/openvla-b2-nproc3-imageAugFalse-0807'\n",
    "vla = AutoModelForVision2Seq.from_pretrained(checkpoint_path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(f\"cuda:{DEVICE_ID}\")\n",
    "\n",
    "# Create Action Tokenizer\n",
    "action_tokenizer = ActionTokenizer(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Use OpenVLA for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MUJOCO_GL=osmesa\n",
    "# os.environ['MUJOCO_GL'] = 'osmesa'\n",
    "# must be set before importing mujoco_py for headless rendering\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from libero.libero.envs import OffScreenRenderEnv, SubprocVectorEnv\n",
    "# from libero.lifelong.metric import (\n",
    "#     evaluate_loss,\n",
    "#     evaluate_success,\n",
    "#     raw_obs_to_tensor_obs,\n",
    "# )\n",
    "# from libero.lifelong.utils import (\n",
    "#     control_seed,\n",
    "#     safe_device,\n",
    "#     torch_load_model,\n",
    "#     NpEncoder,\n",
    "#     compute_flops,\n",
    "# )\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info, extract_env_obs\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_spatial\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "TASK_NAME = \"pick_up_the_black_bowl_next_to_the_cookie_box_and_place_it_on_the_plate\"\n",
    "SAVE_VIDEO = True # save video of the evaluation process\n",
    "EVAL_MAX_STEP = 500 # maximum number of steps for evaluation\n",
    "PARALLEL_ENVS = 1 # number of parallel environments for evaluation\n",
    "TEST_ENV = False # test environment with specific actions\n",
    "if TEST_ENV:\n",
    "    ACTION_PATH = \"../demo_record/actions/KITCHEN_SCENE3_turn_on_the_stove_and_put_the_moka_pot_on_it_demo\"\n",
    "\n",
    "## Check evaluation configureations path\n",
    "BDDL_FILES_BASE_PATH = get_libero_path(\"bddl_files\")\n",
    "INIT_STATES_BASE_PATH = get_libero_path(\"init_states\")\n",
    "VIDEO_FOLDER = \"../eval_record/videos\"\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO evaluation BDDL files path: \", BDDL_FILES_BASE_PATH)\n",
    "print(\"LIBERO evaluation initial states path: \", INIT_STATES_BASE_PATH)\n",
    "if SAVE_VIDEO:\n",
    "    print(\"Video saving to: \", VIDEO_FOLDER)\n",
    "print(\"=====================================\")\n",
    "\n",
    "\n",
    "## Load evaluation environment\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "benchmark_instance = benchmark_dict[DATASET_NAME]()\n",
    "tasks_num_eval = benchmark_instance.get_num_tasks() # number of tasks for evaluation\n",
    "task_names_eval = benchmark_instance.get_task_names() # task names for evaluation\n",
    "print(f\"{tasks_num_eval} tasks for evaluation:\\n {task_names_eval}\")\n",
    "# find the position for TASK_NAME in task_names_eval\n",
    "task_id_eval = task_names_eval.index(TASK_NAME)\n",
    "\n",
    "# load a task for evaluation\n",
    "task_eval = benchmark_instance.get_task(task_id_eval)\n",
    "language_instruction_eval = benchmark_instance.get_task(task_id_eval).language # language instruction for the task\n",
    "print(language_instruction_eval) \n",
    "\n",
    "## Evaluation\n",
    "with Timer() as t, VideoWriter(VIDEO_FOLDER, SAVE_VIDEO) as video_writer:\n",
    "    env_args = {\n",
    "        \"bddl_file_name\": os.path.join(\n",
    "            BDDL_FILES_BASE_PATH, task_eval.problem_folder, task_eval.bddl_file\n",
    "        ),\n",
    "        \"render_gpu_device_id\": DEVICE_ID\n",
    "    }\n",
    "\n",
    "    env = SubprocVectorEnv(\n",
    "        [lambda: OffScreenRenderEnv(**env_args) for _ in range(PARALLEL_ENVS)]\n",
    "    )\n",
    "        \n",
    "    env.reset()\n",
    "    env.seed(0)\n",
    "\n",
    "    init_states_path = os.path.join(\n",
    "        INIT_STATES_BASE_PATH, task_eval.problem_folder, task_eval.init_states_file\n",
    "    )\n",
    "    init_states = torch.load(init_states_path)\n",
    "    indices = np.arange(PARALLEL_ENVS) % init_states.shape[0]\n",
    "    init_states_ = init_states[indices]\n",
    "\n",
    "    dones = [False] * PARALLEL_ENVS\n",
    "    steps = 0\n",
    "    obs = env.set_init_state(init_states_)\n",
    "    \n",
    "    for _ in range(10):  # simulate the physics without any actions\n",
    "            env.step(np.zeros((PARALLEL_ENVS, 7)))\n",
    "    \n",
    "    print(\"Initial RGB observations:\")\n",
    "    fig, ax = plt.subplots()\n",
    "    stacked_image = np.hstack([obs[k][\"agentview_image\"][::-1] for k in range(PARALLEL_ENVS)])\n",
    "    im = ax.imshow(stacked_image)\n",
    "    plt.axis('off')\n",
    "    plt.show(block=False)\n",
    "\n",
    "    ## Testing environment with specific actions\n",
    "    '''\n",
    "    taking 6min for 500 steps\n",
    "    '''\n",
    "    if TEST_ENV:\n",
    "        actions = np.load(ACTION_PATH + \"/0.npy\")\n",
    "        for timestep in range(actions.shape[0]):\n",
    "            print(actions[timestep])\n",
    "            obs, reward, done, info = env.step([actions[timestep]])\n",
    "            video_writer.append_vector_obs(obs, done, camera_name=\"agentview_image\")\n",
    "        env.close()\n",
    "    \n",
    "    ## Use OpenVLA for evaluation\n",
    "    else:\n",
    "        for _ in range(EVAL_MAX_STEP):\n",
    "            # collect image\n",
    "            image = obs[0][\"agentview_image\"][::-1]\n",
    "            image = Image.fromarray(image)\n",
    "            prompt = f\"In: What action should the robot take to {language_instruction_eval}?\\nOut:\"\n",
    "            # Process the image and prompt\n",
    "            with torch.no_grad():\n",
    "                inputs = processor(prompt, image).to(f\"cuda:{DEVICE_ID}\", dtype=torch.bfloat16)\n",
    "                # generate actions using OpenVLA\n",
    "                action = vla.predict_action(**inputs, unnorm_key=\"libero_spatial\", do_sample=False)\n",
    "            action = np.array([action])\n",
    "            # during OpenVLA training, gripper value was converted: -1 -> 1 for open; 1 -> 0 for close\n",
    "            # convert back to original value\n",
    "            # action[0][6] = 1 - 2 * action[0][6]\n",
    "            # step the environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            for k in range(PARALLEL_ENVS):\n",
    "                dones[k] = dones[k] or done[k]\n",
    "            if all(dones):\n",
    "                break\n",
    "            # record video\n",
    "            video_writer.append_vector_obs(obs, done, camera_name=\"agentview_image\")\n",
    "            print(f\"Step: {_} / {EVAL_MAX_STEP}, Action: {action}\")\n",
    "            # visualize every step as image\n",
    "            # vis = np.hstack([obs[k][\"agentview_image\"][::-1] for k in range(PARALLEL_ENVS)])\n",
    "            # im.set_data(vis)\n",
    "            # clear_output(wait=True)  # Clear the previous output\n",
    "            # display(fig)  # Display the updated figure\n",
    "            # plt.pause(0.001)\n",
    "\n",
    "            \n",
    "        env.close()\n",
    "    \n",
    "    # TODO       \n",
    "    # ## Use OpenVLA for evaluation with parallel environments\n",
    "    # for _ in range(EVAL_MAX_STEP):\n",
    "    #     # collect images from all parallel environments\n",
    "    #     images_batch = [obs[k][\"agentview_image\"][::-1] for k in range(PARALLEL_ENVS)]\n",
    "    #     images_batch = [\n",
    "    #         Image.fromarray(image) if isinstance(image, np.ndarray) else image\n",
    "    #         for image in images_batch\n",
    "    #     ]\n",
    "    #     images_batch = [image.convert(\"RGB\") for image in images_batch]\n",
    "    #     # generate prompts based on the language instruction\n",
    "    #     prompts = [f\"In: What action should the robot take to {language_instruction_eval}?\\nOut:\" for _ in range(PARALLEL_ENVS)]\n",
    "    #     # Process the images and prompts\n",
    "    #     inputs = processor(text=prompts, images=images_batch, return_tensors=\"pt\", padding=True).to(\"cuda:2\")\n",
    "    #     for key, value in inputs.items():\n",
    "    #         print(f\"Shape of {key}: {value.shape}\")\n",
    "    #     # generate actions using OpenVLA\n",
    "    #     with torch.no_grad():\n",
    "    #         actions = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "    #     # step the environment\n",
    "    #     obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "    #     # record video\n",
    "    #     video_writer.append_vector_obs(obs, done, camera_name=\"agentview_image\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA_CL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVLA evaluation on LIBERO benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Initialize OpenVLA with Huggingface transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRANSFORMERS_CACHE=/data2/zhaoyu/huggingface_cache\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data2/zhaoyu/huggingface_cache'\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:2\")\n",
    "# print(processor)\n",
    "# print(vla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Use OpenVLA for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MUJOCO_GL=osmesa\n",
    "# os.environ['MUJOCO_GL'] = 'osmesa'\n",
    "# must be set before importing mujoco_py for headless rendering\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "# Add VLA_DIR to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../')))\n",
    "# Add LIBERO to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../external/LIBERO')))\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "from libero.libero.envs import OffScreenRenderEnv, SubprocVectorEnv\n",
    "# from libero.lifelong.metric import (\n",
    "#     evaluate_loss,\n",
    "#     evaluate_success,\n",
    "#     raw_obs_to_tensor_obs,\n",
    "# )\n",
    "# from libero.lifelong.utils import (\n",
    "#     control_seed,\n",
    "#     safe_device,\n",
    "#     torch_load_model,\n",
    "#     NpEncoder,\n",
    "#     compute_flops,\n",
    "# )\n",
    "from utils.LIBERO_utils import get_task_names, extract_task_info, extract_env_obs\n",
    "\n",
    "## User specific configurations\n",
    "# TODO: change this into argparse for user input in python file\n",
    "DATASET_NAME = \"libero_10\" # \"libero_object\", \"libero_spatial\", \"libero_goal\", \"libero_10\", \"libero_90\"\n",
    "SAVE_VIDEO = True # save video of the evaluation process\n",
    "EVAL_MAX_STEP = 500 # maximum number of steps for evaluation\n",
    "DEVICE_ID = 3 # GPU device id for rendering\n",
    "PARALLEL_ENVS = 1 # number of parallel environments for evaluation\n",
    "TEST_ENV = False # test environment with specific actions\n",
    "\n",
    "## Check evaluation configureations path\n",
    "BDDL_FILES_BASE_PATH = get_libero_path(\"bddl_files\")\n",
    "INIT_STATES_BASE_PATH = get_libero_path(\"init_states\")\n",
    "VIDEO_FOLDER = \"../videos\"\n",
    "print(\"=====================================\")\n",
    "print(\"LIBERO evaluation BDDL files path: \", BDDL_FILES_BASE_PATH)\n",
    "print(\"LIBERO evaluation initial states path: \", INIT_STATES_BASE_PATH)\n",
    "if SAVE_VIDEO:\n",
    "    print(\"Video saving to: \", VIDEO_FOLDER)\n",
    "print(\"=====================================\")\n",
    "\n",
    "\n",
    "## Load evaluation environment\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "benchmark_instance = benchmark_dict[DATASET_NAME]()\n",
    "tasks_num_eval = benchmark_instance.get_num_tasks() # number of tasks for evaluation\n",
    "task_names_eval = benchmark_instance.get_task_names() # task names for evaluation\n",
    "print(f\"{tasks_num_eval} tasks for evaluation:\\n {task_names_eval}\")\n",
    "\n",
    "# load a task for evaluation\n",
    "task_id_eval = 0 # TODO: change this into for loop for all tasks\n",
    "task_eval = benchmark_instance.get_task(task_id_eval)\n",
    "language_instruction_eval = benchmark_instance.get_task(task_id_eval).language # language instruction for the task\n",
    "# print(language_instruction_eval)\n",
    "\n",
    "## Evaluation\n",
    "with Timer() as t, VideoWriter(VIDEO_FOLDER, SAVE_VIDEO) as video_writer:\n",
    "    env_args = {\n",
    "        \"bddl_file_name\": os.path.join(\n",
    "            BDDL_FILES_BASE_PATH, task_eval.problem_folder, task_eval.bddl_file\n",
    "        ),\n",
    "        \"render_gpu_device_id\": DEVICE_ID\n",
    "    }\n",
    "\n",
    "    env = SubprocVectorEnv(\n",
    "        [lambda: OffScreenRenderEnv(**env_args) for _ in range(PARALLEL_ENVS)]\n",
    "    )\n",
    "        \n",
    "    env.reset()\n",
    "    env.seed(0)\n",
    "\n",
    "    init_states_path = os.path.join(\n",
    "        INIT_STATES_BASE_PATH, task_eval.problem_folder, task_eval.init_states_file\n",
    "    )\n",
    "    init_states = torch.load(init_states_path)\n",
    "    indices = np.arange(PARALLEL_ENVS) % init_states.shape[0]\n",
    "    init_states_ = init_states[indices]\n",
    "\n",
    "    dones = [False] * PARALLEL_ENVS\n",
    "    steps = 0\n",
    "    obs = env.set_init_state(init_states_)\n",
    "    \n",
    "    for _ in range(10):  # simulate the physics without any actions\n",
    "            env.step(np.zeros((PARALLEL_ENVS, 7)))\n",
    "    \n",
    "    print(\"Initial RGB observations:\")\n",
    "    stacked_image = np.hstack([obs[k][\"agentview_image\"][::-1] for k in range(PARALLEL_ENVS)])\n",
    "    plt.imshow(stacked_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    ## Testing environment with specific actions\n",
    "    '''\n",
    "    taking 6min for 500 steps\n",
    "    '''\n",
    "    if TEST_ENV:\n",
    "        action_temp_1 = np.array([1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 1.0]) # single action for environment 1\n",
    "        action_temp_2 = np.array([5.0, -2.0, 0.0, 0.0, 0.0, 0.0, 1.0]) # single action for environment 2\n",
    "        action_temp_3 = np.array([10.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]) # single action for environment 3\n",
    "        actions = np.array([action_temp_1, action_temp_2, action_temp_3])\n",
    "        for _ in range(20):\n",
    "            obs, reward, done, info = env.step(actions)\n",
    "            video_writer.append_vector_obs(obs, done, camera_name=\"agentview_image\")\n",
    "        env.close()\n",
    "    \n",
    "    ## Use OpenVLA for evaluation\n",
    "    for _ in range(EVAL_MAX_STEP):\n",
    "        # collect image\n",
    "        image = obs[0][\"agentview_image\"][::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        prompt = f\"In: What action should the robot take to {language_instruction_eval}?\\nOut:\"\n",
    "        # Process the image and prompt\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(prompt, image).to(f\"cuda:{2}\", dtype=torch.bfloat16)\n",
    "            # generate actions using OpenVLA\n",
    "            action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "        action = np.array([action])\n",
    "        # step the environment\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # record video\n",
    "        video_writer.append_vector_obs(obs, done, camera_name=\"agentview_image\")\n",
    "        print(f\"Step: {_} / {EVAL_MAX_STEP}, Action: {action}\")\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "    # TODO       \n",
    "    # ## Use OpenVLA for evaluation with parallel environments\n",
    "    # for _ in range(EVAL_MAX_STEP):\n",
    "    #     # collect images from all parallel environments\n",
    "    #     images_batch = [obs[k][\"agentview_image\"][::-1] for k in range(PARALLEL_ENVS)]\n",
    "    #     images_batch = [\n",
    "    #         Image.fromarray(image) if isinstance(image, np.ndarray) else image\n",
    "    #         for image in images_batch\n",
    "    #     ]\n",
    "    #     images_batch = [image.convert(\"RGB\") for image in images_batch]\n",
    "    #     # generate prompts based on the language instruction\n",
    "    #     prompts = [f\"In: What action should the robot take to {language_instruction_eval}?\\nOut:\" for _ in range(PARALLEL_ENVS)]\n",
    "    #     # Process the images and prompts\n",
    "    #     inputs = processor(text=prompts, images=images_batch, return_tensors=\"pt\", padding=True).to(\"cuda:2\")\n",
    "    #     for key, value in inputs.items():\n",
    "    #         print(f\"Shape of {key}: {value.shape}\")\n",
    "    #     # generate actions using OpenVLA\n",
    "    #     with torch.no_grad():\n",
    "    #         actions = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "    #     # step the environment\n",
    "    #     obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "    #     # record video\n",
    "    #     video_writer.append_vector_obs(obs, done, camera_name=\"agentview_image\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA_CL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
